{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading help data...\n"
     ]
    }
   ],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem in $R^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we study the convergence of the Gradient Descent and Newton's method for a larger problem. Feel free, to pick $n$ as you wish and see how this choice impacts the performance.\n",
    "We consider the following minimization problem\n",
    "\n",
    "$$\n",
    "    \\min_{x\\in R^n}  - \\sum_{i=1}^n \\log(1 - x_i^2) - \\sum_{i=1}^n \\log(b_i - a_i^T x)\n",
    "$$\n",
    "where $a_i$ denotes the $i$th row of the sparse matrix $A$ with random coefficients and $b$ is drawn from a uniform distribution (having values in $[0,1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d2f (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code sets up the problem and derivatives.\n",
    "n = 100\n",
    "e = ones(Float64,n)\n",
    "A = sprandn(n,n,3*n./n^2)\n",
    "b = rand(n)\n",
    "\n",
    "f(x) = ((minimum(1-x.^2)>=0) && (minimum(b - A*x)>=0)) ? - dot(e,log(1-x.^2)) - dot(e,log(b - A*x)) : Inf\n",
    "df(x) = 2.*(x./(1-x.^2))+A'*(1./(b-A*x))  \n",
    "d2f(x) =  2* spdiagm((x.^2 + 1)./(x.^2-1).^2,0,n,n) + A'*spdiagm(1./((b-A*x).^2),0,n,n)*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now apply Newton's method (with fixed line search of $1$) to the problem. The function we call also is contained in ``optiFuncs.jl``. The function looks like\n",
    "```\n",
    "function newton(f::Function,df::Function,H::Function,x::Vector;maxIter=20,atol=1e-8,doPrint=false)\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return x,his,X\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(\"/home/juser/math346/optiFuncs.jl\")\n",
    "\n",
    "x0 = zeros(Float64,n)\n",
    "xnt,hisnt = newton(f,df,d2f,x0,doPrint=false,maxIter=20,atol=1e-20);\n",
    "\n",
    "subplot(1,2,1)\n",
    "semilogy(hisnt[:,1]-hisnt[end,1])\n",
    "title(\"suboptimality, f(xk) - p*\")\n",
    "xlabel(\"iteration\")\n",
    "subplot(1,2,2)\n",
    "semilogy(hisnt[:,2])\n",
    "title(\"norm of gradient, |df(x_k)|\")\n",
    "xlabel(\"iteration\")\n",
    "\n",
    "@printf \"solution has function value %1.15f\" f(xnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the function ``gd`` in the file ``optiFuncs.jl``. Make sure to include it. The function looks like this\n",
    "\n",
    "```\n",
    "function gd(f::Function,df::Function,x::Vector;maxIter=20,atol=1e-8,doPrint=false,ls::Function=armijo)\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    return x,his,X\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "n not defined\nwhile loading In[1], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "n not defined\nwhile loading In[1], in expression starting on line 2",
      ""
     ]
    }
   ],
   "source": [
    "myLS(f,fc,df,x,pk) =  armijo(f,fc,df,x,pk,alpha=1e-1,b=0.9,maxIter=2000)\n",
    "x0 = zeros(Float64,n)\n",
    "xgd,hisgd,Xgd = gd(f,df,x0,maxIter=100,doPrint=false,ls=myLS);\n",
    "\n",
    "subplot(1,2,1)\n",
    "semilogy(hisgd[:,1]-hisnt[end,1])\n",
    "title(\"suboptimality, f(xk) - p*\")\n",
    "xlabel(\"iteration\")\n",
    "subplot(1,2,2)\n",
    "semilogy(hisgd[:,2])\n",
    "title(\"norm of gradient, |df(x_k)|\")\n",
    "xlabel(\"iteration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.6",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
